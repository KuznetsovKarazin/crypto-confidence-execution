# Cryptocurrency Confidenceâ€‘Based Execution System

A productionâ€‘ready ML system for cryptocurrency priceâ€‘movement prediction with **confidenceâ€‘based execution**. It combines highâ€‘frequency orderâ€‘book microstructure with macro OHLCV features, optimizes **profitâ€‘oriented thresholds (Ï„)**, and now includes **federated learning** and **privacyâ€‘preserving secure aggregation (Shamir + DP)**.

---

## ğŸ¯ Overview

This system implements a **twoâ€‘class trading approach** that:

- **Predicts direction** (Up vs Down) with neural models.
- **Executes only when confidence â‰¥ Ï„**, optimizing Ï„ for **profit**, **expected value (profit Ã— coverage)**, or **profit with minimum coverage**.
- **Accounts for trading costs** in all threshold searches.
- **Provides centralized, federated, and privacyâ€‘preserving pipelines**, plus robust analysis & visualization utilities.

### Whatâ€™s new (2025â€‘10)

- **Federated training runner** (`run_federated.py`) with **FedAvg / Deltaâ€‘FedAvg / FedAvgM** and **robust aggregators** (coordinateâ€‘median, trimmedâ€‘mean).
- **Privacyâ€‘preserving FL** (`run_privacy_federated.py`) integrating **Shamir Secret Sharing** + **Differential Privacy** (RDPâ€‘inspired composition) via `enhanced_shamir_privacy.py`.
- **Universal model comparator** (`compare_multiple_models.py`) â€” align predictions from N models and sweep Ï„ **without retraining**.
- **Ï„â€‘curve plotting** (`plot_tau_curves.py`) and **sweep aggregator** (`sweep_results_aggregator.py`).
- **Federated preprocessing** (`federated/preprocessing.py`) that strictly aligns to centralized artifacts to prevent featureâ€‘mismatch.
- **Cleaner logs & configs**, symbolâ€‘aware temporal splits, twoâ€‘class selective execution metrics, and CLI â€œcheatâ€‘sheetâ€.

---

## ğŸ—ï¸ Repository Structure

```
crypto-confidence-execution/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ macro/                         # Daily OHLCV
â”‚   â”œâ”€â”€ micro/                         # Order-book snapshots
â”‚   â””â”€â”€ processed/                     # Unified datasets
â”œâ”€â”€ data_preprocessing.py              # Build unified_dataset.parquet
â”œâ”€â”€ centralized_training_two_classes.py# Centralized training (two-class, Ï„-optimized)
â”œâ”€â”€ sweep_two_class.py                 # Grid sweeps (horizon, deadband, Ï„ criteria)
â”œâ”€â”€ sweep_results_aggregator.py        # Aggregate + visualize sweeps
â”œâ”€â”€ plot_tau_curves.py                 # Profit/coverage curves vs Ï„ (no retraining)
â”œâ”€â”€ compare_multiple_models.py         # Align & compare N models across Ï„
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ centralized/                   # Centralized runs & sweeps
â”‚   â”œâ”€â”€ federated/                     # Federated runs
â”‚   â””â”€â”€ privacy/                       # Privacy-preserving federated runs
â”œâ”€â”€ federated/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core.py                        # Trainer + loop
â”‚   â”œâ”€â”€ client.py                      # Client logic (balanced batches, best-val model)
â”‚   â”œâ”€â”€ aggregation.py                 # FedAvg, Delta-FedAvg, FedAvgM, robust aggregators
â”‚   â”œâ”€â”€ preprocessing.py               # Load centralized artifacts, strict feature alignment
â”‚   â””â”€â”€ utils.py                       # Data manager, shapes/cleaning, config helpers
â”œâ”€â”€ run_federated.py                   # Simple federated runner
â”œâ”€â”€ run_privacy_federated.py           # Federated + Shamir + DP
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ enhanced_shamir_privacy.py     # Shamir, secure aggregation, DP config
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1) Data Preparation

- **Macro data**: daily OHLCV (CSV)
- **Micro data**: orderâ€‘book snapshots (CSV)

Place as:
```
data/macro/top_100_cryptos_with_correct_network.csv
data/micro/*.csv
```

Build the unified dataset:
```bash
python data_preprocessing.py
# â†’ data/processed/unified_dataset.parquet
```

### 2) Centralized Training (Twoâ€‘Class)

Single run:
```bash
python centralized_training_two_classes.py \
  --unified data/processed/unified_dataset.parquet \
  --out experiments/centralized \
  --task classification \
  --model mlp \
  --horizon_min 600 --deadband_bps 10 \
  --two_class_mode \
  --use_robust_scaler \
  --calibrate_probabilities \
  --confidence_tau 0.80 \
  --profit_cost_bps 1.0 \
  --top_k_features 128 \
  --batch_size 1024 --epochs 20 --seed 42
```

Parameter sweep (Ï„ optimization **without** retraining the model each time):
```bash
python sweep_two_class.py \
  --unified data/processed/unified_dataset.parquet \
  --out experiments/centralized \
  --horizons 300 600 900 \
  --deadbands 5 10 15 \
  --top_k_features 64 \
  --profit_cost_bps 1.0 \
  --grid 0.50:0.98:0.01 \
  --optimize_tau_by profit ev profit_with_min_coverage \
  --min_coverage 0.005 \
  --use_robust_scaler --calibrate
```

Analyze sweeps:
```bash
python sweep_results_aggregator.py --base_dir experiments/centralized
```

Plot Ï„â€‘curves for a single predictions CSV (no retraining):
```bash
python plot_tau_curves.py \
  --predictions experiments/centralized/.../test_predictions_two_class.csv \
  --cost_bps 1.0 --out plots/tau_curves
```

### 3) Federated Learning

Use centralized artifacts (imputer, scaler, feature schema, optional `decision_threshold.json`) for strict feature alignment.

Basic run:
```bash
python run_federated.py \
  --federated_data data/processed_federated_iid \
  --centralized_artifacts experiments/centralized/<BEST_RUN>/ \
  --output experiments/federated \
  --aggregation delta_fedavg \
  --rounds 30 --local_epochs 5 \
  --server_lr 0.5 --client_lr 0.01 \
  --batch_size 1024 \
  --horizon 600 --deadband 10.0 \
  --two_class --confidence_tau 0.80
```

Available aggregations:
- `fedavg`, `delta_fedavg`, `fedavgm` (+ `--server_momentum`), `coordinate_median`, `trimmed_mean`.

### 4) Privacyâ€‘Preserving Federated Learning

Enable **Shamir secret sharing** and **differential privacy** atop the federated runner:

```bash
python run_privacy_federated.py \
  --federated_data data/processed_federated_iid \
  --centralized_artifacts experiments/centralized/<BEST_RUN>/ \
  --output experiments/privacy \
  --aggregation delta_fedavg \
  --rounds 15 --local_epochs 3 \
  --server_lr 0.5 --client_lr 0.01 \
  --batch_size 1024 \
  --horizon 600 --deadband 10.0 \
  --two_class --confidence_tau 0.80 \
  --privacy --shamir --dp \
  --threshold 3 --prime_bits 61 \
  --epsilon 1.0 --delta 1e-6 --clip_norm 1.0 --per_layer_dp
```

- `utils/enhanced_shamir_privacy.py` implements: vectorized encoding/decoding, centered modular arithmetic, seed derivation for secure aggregation, dropout recovery via seed shares, and an RDPâ€‘inspired perâ€‘round Îµ schedule.
- `test_privacy_integration.py` provides quick checks for **Shamir correctness**, **DP noise**, **mask cancellation**, and **dropout recovery**.

---

## ğŸ“Š Metrics & Selective Execution

On executed trades (confidence â‰¥ Ï„), we report:
- **Coverage** (% of samples executed)
- **Direction Accuracy**
- **Average/Median Profit (bps)** after costs
- **Win Rate**
- **Sharpe** and **EV per sample** (avg_profit Ã— coverage)

Centralized training also logs class balances, split ranges, and featureâ€‘selection summaries. Federated runs log perâ€‘round validation/test accuracy and aggregation stats; privacy runs add DP/Shamir diagnostics.

---

## ğŸ”¬ Comparing Models Without Retraining

Align predictions and sweep Ï„ on **multiple** models at once:
```bash
python compare_multiple_models.py \
  --model centralized=experiments/centralized/.../test_predictions_two_class.csv \
  --model federated=experiments/federated/.../predictions_on_centralized_test.csv \
  --model privacy=experiments/privacy/.../predictions_on_centralized_test.csv \
  --cost_bps 1.0 \
  --tau_min 0.50 --tau_max 0.90 --tau_step 0.005 \
  --out_dir model_comparison
```
Generates profitâ€‘vsâ€‘coverage tradeâ€‘off curves, optimal Ï„ markers, and summary tables per model.

---

## âš™ï¸ Key Configuration (centralized)

| Parameter              | Description                                   | Default |
|------------------------|-----------------------------------------------|---------|
| `horizon_min`          | Prediction horizon (minutes)                  | 600     |
| `deadband_bps`         | Minimum movement threshold (bps)              | 10      |
| `two_class_mode`       | Binary Up/Down training                       | true    |
| `confidence_tau`       | Execution confidence threshold                | 0.70    |
| `profit_cost_bps`      | Trading cost per trade (bps)                  | 1.0     |
| `top_k_features`       | Feature selection (MI/Fâ€‘score/variance)       | 128     |
| `use_robust_scaler`    | Robust vs standard scaling                    | true    |
| `calibrate_probabilities` | Platt/Isotonic via `CalibratedClassifierCV`| true    |
| `optimize_tau_by`      | `profit` \| `ev` \| `profit_with_min_coverage`| profit  |
| `min_coverage`         | Min coverage for constrained optimization     | 0.0     |

### Federated notes
- **Deltaâ€‘FedAvg** applies server LR and gradient clipping to client **deltas**.
- **FedAvgM** uses server momentum; robust options: **coordinateâ€‘median**, **trimmedâ€‘mean**.
- **Feature alignment** uses centralized artifacts to ensure identical dimension/order on every client.

---

## ğŸ“ˆ Example Results (illustrative)

```
Coverage: 15.2%  | Direction Acc: 0.682
Avg Profit: 3.47 bps (after 1.0 bps cost)
Win Rate: 58.3%  | Sharpe: 1.24
```
Use the **sweep aggregator** + **Ï„â€‘curves** to find the desired profit/coverage regime (e.g., Ï„=0.60 vs Ï„=0.80).

---

## ğŸ” Reproducibility & Seeds

- Deterministic seeds for data splits and batching.
- Symbolâ€‘aware temporal splits to prevent leakage.
- Training size capping uses **proportional random sampling** per symbol.

---

## ğŸ§© Troubleshooting (quick)

- **Feature mismatch** in federated runs â†’ ensure the federated preprocessor loads the **same** centralized `imputer.joblib`, `scaler.joblib`, and `feature_schema.json`.
- **Too few trade samples** in twoâ€‘class mode â†’ relax `--deadband_bps` or increase dataset window.
- **Low coverage after Ï„ optimization** â†’ use `--optimize_tau_by profit_with_min_coverage --min_coverage 0.005`.
- **FedAvgM oscillations** â†’ reduce `--server_momentum` or use `delta_fedavg` with smaller `--server_lr`.
- **Unicode logs on Windows** â†’ runners set UTFâ€‘8â€‘safe console/file handlers.

---

## ğŸ§ª Dev Utilities

- **Grid search (federated)**: see `hyperparameter_search.py` for scripted exploration of rounds/epochs/batch/LRs/aggregations.
- **Privacy tests**: `test_privacy_integration.py` for Shamir/DP/secureâ€‘aggregation sanity checks.

---

## ğŸ—‚ï¸ Results Layout (per run)

- `config.json` / `config.yaml` â€” full parameters
- `training.log` / `federated_training.log` â€” structured logs
- `metrics_two_class.json` / `metrics_global.json` â€” core KPIs
- `test_predictions_two_class.csv` â€” perâ€‘sample outputs
- `decision_threshold.json` â€” selected Ï„ & validation stats
- `plots/` â€” Ï„â€‘curves and summary figures (if enabled)

---

## ğŸ§­ CLI Cheatâ€‘Sheet

```bash
# Centralized (single)
python centralized_training_two_classes.py ...

# Centralized sweeps (+ aggregation/plots later)
python sweep_two_class.py ... && python sweep_results_aggregator.py --base_dir experiments/centralized

# Ï„ curves on an existing predictions CSV
python plot_tau_curves.py --predictions <csv> --cost_bps 1.0 --out plots/tau_curves

# Federated (no privacy)
python run_federated.py --aggregation delta_fedavg --rounds 30 --local_epochs 5 ...

# Federated with privacy (Shamir + DP)
python run_privacy_federated.py --privacy --shamir --dp --threshold 3 --epsilon 1.0 ...
```

---

**License & Citation**: If you use this codebase or results in academic/industrial work, please include a reference to the repository and acknowledge the confidenceâ€‘based selectiveâ€‘execution methodology and federated/privacy modules.
